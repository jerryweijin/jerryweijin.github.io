[
  {
    "objectID": "how_to_build_a_multi_label_classifier.html",
    "href": "how_to_build_a_multi_label_classifier.html",
    "title": "How to Create a Multi-label Classifier",
    "section": "",
    "text": "The first problem our pet breed classifier has is that it can’t output more than one class if an image contains more than one pet breed.\n\n\n\ndog_and_cat.png\n\n\nThe second problem is the classifier will always pick a class from the 37 pet breeds even if an image is not a pet.\n\n\n\ncar.png\n\n\nTo solve these problems, we need a multi-label classifier. Differ from a multi-class classifier a multi-label classifier can outputs more than one class per input exmaple. To train a multi-label classfier, we need to load a different dataset as the pet breed dataset only has one label per image.\n\nfrom fastai.vision.all import *\ndata_path = untar_data(URLs.PASCAL_2007)\ndata_path\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  Expected in: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\nPath('/Users/jerryyu/.fastai/data/pascal_2007')\n\n\nAccording to the PASCAL website, the 2007 dataset contains 20 classes and 9963 images splitted into train/validation/test set. These 20 classes are:\n\nPerson: person\nAnimal: bird, cat, cow, dog, horse, sheep\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n\nThe dataset is structured like so\n\n\n\nPASCAL2007.png\n\n\nTraining and validation images are stored in the train folder and test images are stored in the test folder. The train.csv and test.csv files contain labels for each image. Please take a look at how an image is labeled.\n\nimport pandas as pd\ndf = pd.read_csv(data_path/'train.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n\n\nThe labels are stored in a labels column. If an imange has more than one label, each label is separated by a space. The is_valid column tells whether an image belongs to the training set or validation set. Because this dataset was created for a challange. The creator wants to make sure every competitor uses the same split for the compitition. Let’s use the DataBlock API to build our dataloaders step by step.\n\ndb = DataBlock()\ndata = db.datasets(df)\nx, y = data.train[0]\nx,y\n\n(fname       006852.jpg\n labels      sofa chair\n is_valid         False\n Name: 3456, dtype: object,\n fname       006852.jpg\n labels      sofa chair\n is_valid         False\n Name: 3456, dtype: object)\n\n\nfastai creates a training and validation set from the train.csv file. Right now x, y are the same because we didn’t tell fastai how to create our inputs and targets from this file. By default, fastai just set the input and target of each example to be the the value of the entire row. We need to write a function to extract the labels from the labels column and split them by a space. We also need a function to get the path of each image.\n\ndef get_labels(row):\n    return row['labels'].split()\n\ndef get_fname(row):\n    return data_path/'train'/row['fname']\n\n\nassert get_labels(df.iloc[2]) == ['horse', 'person']\nassert get_fname(df.iloc[0]) ==  (data_path/'train'/'000005.jpg')\n\nLet’s rebuild our dataset and see how x and y look like now\n\ndata = DataBlock(get_x=get_fname, get_y=get_labels).datasets(df)\nx, y = data.train[0]\nx,y\n\n(Path('/Users/jerryyu/.fastai/data/pascal_2007/train/003537.jpg'), ['train'])\n\n\nNow we have the labels and the file path but these are not the things we feed into our model. For a multi-label image classifier, we need the inputs to be image tensors and targets to be one-hot encoded tensors. We can use the blocks parameter to tell fastai to do the conversion for us.\n\ndata = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_x=get_fname, \n    get_y=get_labels\n).datasets(df)\nx, y = data.train[0]\nx,y\n\n(PILImage mode=RGB size=500x342,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 1., 0., 0.]))\n\n\nNote that x and y are not tensors. In fastai, there are three types of transform: type transform, item transform, and batch transform.\nType transform is the base level transform. It takes the source data and transform data into the format suitable for traning our model. In our example, type transfrom takes our csv file and transfrom the data to PILImage (for the input) and one-hot encoded category (for the target).\nItem transform is performed after type transform. It transfroms whatever we get from type transfrom to tensors. You can also define other item transformations by passing these transformation to the item_tfms parameter in the DataLock method.\nBatch transform is performed after item transform. It applies transformations to the whole batch. Batch transform includes Cuda, IntToFloatTensor and any custom transfrom you pass to the batch_tfms parameter.\nIn our example, we will use item transform to make all the images the same size and batch transform to do augmentation.\n\ndata = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_x=get_fname, \n    get_y=get_labels,\n    item_tfms = Resize(460),\n    batch_tfms = aug_transforms(size=224, min_scale=0.75)\n).datasets(df)\nx, y = data.train[0]\nx,y\n\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0.]))\n\n\nOur dataset looks good now. Let’s create our dataloaders.\n\ndls = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_x=get_fname, \n    get_y=get_labels,\n    item_tfms = Resize(460),\n    batch_tfms = aug_transforms(size=224, min_scale=0.75)\n).dataloaders(df)\n\ndls.show_batch(max_n=3)\n\n\n\n\nNow our dataloaders is ready. To create our learner, we need 4 things: dataloaders, model, metrics, and loss function. We already have our dataloaders. fastai can provide the model and loss function. All is left is to define our metrics. We can’t use the error_rate metrics because we are dealing with a multi-label classifier. We need to define our own way of calculating accuracy (error rate).\n\nimport torch\ndef multi_label_accuracy(output, target, threshold=0.5, sigmoid=True):\n    '''\n    If sigmoid is true, output is logits with size bs * n_classes.\n    If sigmoid is false, output is probability with size bs * n_classes.\n    Target is a one-hot encoded tensor with size bs * n_classes. \n    '''\n    if sigmoid == True: \n        pred = torch.sigmoid(output)\n    else:\n        pred = output\n    return ((pred &gt; threshold) == target).float().mean()\n\n\nassert multi_label_accuracy(torch.tensor([-4, 0.3, 0.2, -1]), torch.tensor([0,1,0,1])) == 0.5\n\nNow everything is ready, we can create our learner to train our model\n\nlearner = vision_learner(dls, resnet50, metrics=partial(multi_label_accuracy, threshold=0.5))\nlearner.fine_tune(2)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmulti_label_accuracy\ntime\n\n\n\n\n0\n0.786185\n0.456157\n0.810230\n22:43\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmulti_label_accuracy\ntime\n\n\n\n\n0\n0.423361\n0.210357\n0.959780\n25:11\n\n\n1\n0.271580\n0.145638\n0.970858\n28:34\n\n\n\n\n\nWith threshold = 0.5, our accuracy is 97%. The threshold is a hyperparameter we need to choose for our model. Let’s see how varying the threshold can change our model’s accuracy. First we get the predictions of our validaiton set by using the get_preds method. The preditions returned by get_preds is probability so we need to set sigmoid = False when passing the predictions to our accurancy funciton.\n\npreds, targets = learner.get_preds()\n\n\n\n\n\n\n\n\n\nmulti_label_accuracy(preds, targets, threshold = 0.8, sigmoid=False)\n\nTensorBase(0.9725)\n\n\n\nmulti_label_accuracy(preds, targets, threshold = 0.3, sigmoid=False)\n\nTensorBase(0.9554)\n\n\nTo pick the threshold that yields the best accuracy, we can plot a curve of accuracy at every threshold .\n\nx = torch.linspace(0,1,30)\naccuracy = [multi_label_accuracy(preds, targets, threshold = i, sigmoid=False) for i in x]\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,4))\nplt.plot(x, accuracy)\n\n\n\n\nFrom this curve, thresholds above 0.4 barely change the accuracy, so any value betwwen 0.4 and 0.9 would work.\n\nlearner.export('model.pkl')"
  },
  {
    "objectID": "how_to_build_a_multi_class_classifier.html",
    "href": "how_to_build_a_multi_class_classifier.html",
    "title": "How to Create a Multi-class Image Classifier",
    "section": "",
    "text": "We can download the pet breeds dataset using fastai\n\nfrom fastai.vision.all import *\n\nModuleNotFoundError: No module named 'fastprogress'\n\n\n\npath = untar_data(URLs.PETS)\npath\n\nPath('/Users/jerryyu/.fastai/data/oxford-iiit-pet')\n\n\nThe dataset was downlaoded to a folder named oxford-iiit-pet. Let’s see how this dataset is structured. \nOur data (images) is stored in the images folder. We need to change our path to point to our data. The annotations folder contains more infomation of this dataset but we won’t be using it in this tutorial.\n\npath = path / 'images'\npath\n\nPath('/Users/jerryyu/.fastai/data/oxford-iiit-pet/images')"
  },
  {
    "objectID": "how_to_build_a_multi_class_classifier.html#collect-data",
    "href": "how_to_build_a_multi_class_classifier.html#collect-data",
    "title": "How to Create a Multi-class Image Classifier",
    "section": "",
    "text": "We can download the pet breeds dataset using fastai\n\nfrom fastai.vision.all import *\n\nModuleNotFoundError: No module named 'fastprogress'\n\n\n\npath = untar_data(URLs.PETS)\npath\n\nPath('/Users/jerryyu/.fastai/data/oxford-iiit-pet')\n\n\nThe dataset was downlaoded to a folder named oxford-iiit-pet. Let’s see how this dataset is structured. \nOur data (images) is stored in the images folder. We need to change our path to point to our data. The annotations folder contains more infomation of this dataset but we won’t be using it in this tutorial.\n\npath = path / 'images'\npath\n\nPath('/Users/jerryyu/.fastai/data/oxford-iiit-pet/images')"
  },
  {
    "objectID": "how_to_build_a_multi_class_classifier.html#train-our-model",
    "href": "how_to_build_a_multi_class_classifier.html#train-our-model",
    "title": "How to Create a Multi-class Image Classifier",
    "section": "Train Our Model",
    "text": "Train Our Model\nNow our path is correct, we can construct our dataloaders.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n    item_tfms = Resize(460),\n    batch_tfms = aug_transforms(size=224, min_scale=0.75)\n).dataloaders(path, bs=32)\n\nThe blocks arguments tell fastai what input our model needs and what output our model will produce. Here our model will accept images as input and will produce a categorical output. get_items tells how to find our data. We use the get_image_file method to get all the file paths. get_y tells how to get the label from our file path. We grab the class name from the file name. Then we apply item transform and batch transform to our data before feed it ot our model. Let’s see a few images from our dataset.\n\ndls.show_batch(max_n=3)\n\n\n\n\nNow our data is ready, we can build a learner to train our model.\n\nlearner = vision_learner(dls, resnet34, metrics=error_rate)\nlearner.fine_tune(2)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.063441\n0.328220\n0.113667\n19:54\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.590826\n0.358716\n0.109608\n24:31\n\n\n1\n0.318244\n0.248668\n0.077131\n25:47\n\n\n\n\n\nThe result looks pretty good. Let’s take a look at the confusion matrix\n\ninterp = ClassificationInterpretation.from_learner(learner)\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(3, nrows=3)"
  },
  {
    "objectID": "how_to_build_a_multi_class_classifier.html#host-our-model",
    "href": "how_to_build_a_multi_class_classifier.html#host-our-model",
    "title": "How to Create a Multi-class Image Classifier",
    "section": "Host Our Model",
    "text": "Host Our Model\nLet’s export the model and host it on Hugging Face. The tutorial talks about hosting model on Hugging Face so I won’t repeat here.\n\nlearner.export('model.pkl')"
  },
  {
    "objectID": "how_to_create_a_binary_class_classifier.html",
    "href": "how_to_create_a_binary_class_classifier.html",
    "title": "How to Create an Image Classifer",
    "section": "",
    "text": "To train our classfier, we need images of Labrador and Golden Retriever. Luckily, we can easily get the images from the web. We will use DockDockGo to download the images.\n\nfrom duckduckgo_search import ddg_images\n\nLet’s try using ddg_images to download an image and see what it returns.\n\nresult = ddg_images('Labrador', max_results=1)\nresult\n\n[{'title': 'British Labrador - 15 fascinating facts that you should know - DOGS & CATS HQ',\n  'image': 'https://dogsandcatshq.com/wp-content/uploads/2020/08/labrador-retriever-4446566_1920-1536x1152.jpg',\n  'thumbnail': 'https://tse3.mm.bing.net/th?id=OIP.a5F_IVmQ42Zc7CwzsIDLXAHaFj&pid=Api',\n  'url': 'https://dogsandcatshq.com/15-facts-to-know-about-british-labrador-1593',\n  'height': 1152,\n  'width': 1536,\n  'source': 'Bing'}]\n\n\nWe got a list back. This list contains one dictionary represeting one image. The part we need is the image URL. fastai’s L class is like a python list but adds useful functions to make working with list easier. One of the useful function is itemgot which allows you to get the image URL from the dictionary.\n\nfrom fastcore.all import *\nURL = L(result).itemgot('image')\nURL\n\n(#1) ['https://dogsandcatshq.com/wp-content/uploads/2020/08/labrador-retriever-4446566_1920-1536x1152.jpg']\n\n\nNow we have the URL, we can download the image and see what it looks like. fastdownload provides a useful function called download_url which allows you to download a file from the Internet. We will use this function to download the image.\n\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Labrador/lab.jpg')\ndownload_url(URL[0], dest)\nImage.open(dest).to_thumb(256,256)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  Expected in: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\n\n\n\n\n    \n      \n      100.55% [155648/154793 00:00&lt;00:00]\n    \n    \n\n\n\n\n\nWe created a path for the downloaded image and passed the path and the image URL to download_url. Once the image is downloaded, we used Image from PIL to open the image. The to_thumb function is added by fastai to resize an image. I used it to make the image smaller so the output takes less space. The image looks good. We can use the same procedure to download more Labrador and Golden Retriever iamges. To make our life easier, we will create a function to combine ddg_images and extracting the image URLs.\n\ndef search_images(term, max_results=50):\n    return L(ddg_images(term, max_results = max_results)).itemgot('image')\n\nNow we will download more images\n\n# First download Labrador images\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Labrador')\ndest.mkdir(exist_ok=True, parents=True)\ndownload_images(dest, urls=search_images('Labrador', max_results=100))\n\n# Then download Golden Retriever images\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Golden_Retriever')\ndest.mkdir(exist_ok=True, parents=True)\ndownload_images(dest, urls=search_images('Golden Retriever', max_results=100))\n\nInstead of using download_url to download the images, I used download_images. download_images is a function from fastai to download a list of images into a directory. Since we are dealing with many images, using download_images is more convenient. download_images only works if the destination directory already exists hence we need to create the directory before calling download_iamges. Passing parents=True to create all the neccessory parent directories. Passing exist_ok = True to avoid getting an exception if the directories already exsit. I put all the Labrador images into the Labrador directory and all Golden Retriever images into a Golden_Retriever directory. This is a common way of organizing data and will make our traning job easier as fastsi provides ways to label our data if we organize our data this way.\nLet’s do a sanity check to make sure we don’t have broken images.\n\npath = Path('../images/Labrador_and_Golden_Retriever_Classifier')\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\nHere we check for any broken images and remove them. Looks like we have one image that can’t be open. Luckily that we have removed it from our dataset."
  },
  {
    "objectID": "how_to_create_a_binary_class_classifier.html#collect-data",
    "href": "how_to_create_a_binary_class_classifier.html#collect-data",
    "title": "How to Create an Image Classifer",
    "section": "",
    "text": "To train our classfier, we need images of Labrador and Golden Retriever. Luckily, we can easily get the images from the web. We will use DockDockGo to download the images.\n\nfrom duckduckgo_search import ddg_images\n\nLet’s try using ddg_images to download an image and see what it returns.\n\nresult = ddg_images('Labrador', max_results=1)\nresult\n\n[{'title': 'British Labrador - 15 fascinating facts that you should know - DOGS & CATS HQ',\n  'image': 'https://dogsandcatshq.com/wp-content/uploads/2020/08/labrador-retriever-4446566_1920-1536x1152.jpg',\n  'thumbnail': 'https://tse3.mm.bing.net/th?id=OIP.a5F_IVmQ42Zc7CwzsIDLXAHaFj&pid=Api',\n  'url': 'https://dogsandcatshq.com/15-facts-to-know-about-british-labrador-1593',\n  'height': 1152,\n  'width': 1536,\n  'source': 'Bing'}]\n\n\nWe got a list back. This list contains one dictionary represeting one image. The part we need is the image URL. fastai’s L class is like a python list but adds useful functions to make working with list easier. One of the useful function is itemgot which allows you to get the image URL from the dictionary.\n\nfrom fastcore.all import *\nURL = L(result).itemgot('image')\nURL\n\n(#1) ['https://dogsandcatshq.com/wp-content/uploads/2020/08/labrador-retriever-4446566_1920-1536x1152.jpg']\n\n\nNow we have the URL, we can download the image and see what it looks like. fastdownload provides a useful function called download_url which allows you to download a file from the Internet. We will use this function to download the image.\n\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Labrador/lab.jpg')\ndownload_url(URL[0], dest)\nImage.open(dest).to_thumb(256,256)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  Expected in: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\n\n\n\n\n\n    \n      \n      100.55% [155648/154793 00:00&lt;00:00]\n    \n    \n\n\n\n\n\nWe created a path for the downloaded image and passed the path and the image URL to download_url. Once the image is downloaded, we used Image from PIL to open the image. The to_thumb function is added by fastai to resize an image. I used it to make the image smaller so the output takes less space. The image looks good. We can use the same procedure to download more Labrador and Golden Retriever iamges. To make our life easier, we will create a function to combine ddg_images and extracting the image URLs.\n\ndef search_images(term, max_results=50):\n    return L(ddg_images(term, max_results = max_results)).itemgot('image')\n\nNow we will download more images\n\n# First download Labrador images\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Labrador')\ndest.mkdir(exist_ok=True, parents=True)\ndownload_images(dest, urls=search_images('Labrador', max_results=100))\n\n# Then download Golden Retriever images\ndest = Path('../images/Labrador_and_Golden_Retriever_Classifier/Golden_Retriever')\ndest.mkdir(exist_ok=True, parents=True)\ndownload_images(dest, urls=search_images('Golden Retriever', max_results=100))\n\nInstead of using download_url to download the images, I used download_images. download_images is a function from fastai to download a list of images into a directory. Since we are dealing with many images, using download_images is more convenient. download_images only works if the destination directory already exists hence we need to create the directory before calling download_iamges. Passing parents=True to create all the neccessory parent directories. Passing exist_ok = True to avoid getting an exception if the directories already exsit. I put all the Labrador images into the Labrador directory and all Golden Retriever images into a Golden_Retriever directory. This is a common way of organizing data and will make our traning job easier as fastsi provides ways to label our data if we organize our data this way.\nLet’s do a sanity check to make sure we don’t have broken images.\n\npath = Path('../images/Labrador_and_Golden_Retriever_Classifier')\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n1\n\n\nHere we check for any broken images and remove them. Looks like we have one image that can’t be open. Luckily that we have removed it from our dataset."
  },
  {
    "objectID": "how_to_create_a_binary_class_classifier.html#train-our-model",
    "href": "how_to_create_a_binary_class_classifier.html#train-our-model",
    "title": "How to Create an Image Classifer",
    "section": "Train Our Model",
    "text": "Train Our Model\nNow we have our data, the next step is to train our model. We need a dataloader that can feed data to our model in batches. The most flexible way to create a dataloader is to using a DataBlock\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = parent_label,\n    splitter = RandomSplitter(valid_pct=0.2, seed=30),\n    item_tfms = Resize(460),\n    batch_tfms = aug_transforms(size=224, min_scale=0.75)\n).dataloaders(path, bs=32)\n\nThere is a lot of information above, so let’s unpack it.\nWe created our dataloaders from a DataBlock. This is the most flexible way to create a dataloaders object. DataBlock is a call from fastai. You can think of a Datablock as a Dataset. In fact, DataBlock is a class that wraps around PyTorch’s Dataset class. The blocks parameter tells the input data type and output data type of our model. Here our input is images so we use ImageBlock. Our output is a categorical label (e.g. Labrador or Golder Retriever), so we use CategoryBlock.\nget_items tells fastai how to find our data. Here we use the get_image_files function which is also provided by fastai to get all the images recursively in the directory defined by our path variable.\nSimilarly get_y tells fastai how we want to label our data. Passing parent_label means we get the label for each image from the parent directory. Now you see why we organized our Labrador images and Golden Retriever images into the Labrador and Golder_Retriever directory because it allows us to easily get the label of our data.\nspliter defines how we want to split our data. Here we use a 80/20 percent split (80% training data and 20% validation data). Setting the seed allows us to get the same split every time.\nitem_tfms allows us to do preprocessing on each image before passing to our model. Our model expects every image to be the same size but our images are downloaded from the Internet hence their sizes are different. Our final desired image size is 224 px x 224 px but we resized our image to a larger size (460 px x 460 px) so that when performing augmentation, we won’t have any empty space.\nbatch_tfms performs augmentation on a whole batch. We use augmentation to ulter our images (e.g. rotate, zoom, warp, etc) so that our model can recognize images even if they are not perfectly taken. fastai performs all the transforms together with a single interpolation at the end to improve the quality of the augmented images. Doing augmentation on the whole batch instead of on each image one by one can improve the performance.\nLastly, after our DataBlock is define, we used the dataloaders function to create our dataloades. Note the plural form in the function name. The dataloaders object actually have two dataloaders: one for the training data and one for the validation data.\nLet’s see some images from our dataloaders.\n\ndls.show_batch(max_n=3)\n\n\n\n\nLooking good. Now we have our dataloaders. It is time to create our learner and train our model.\n\nlearner = vision_learner(dls, resnet18, metrics=error_rate)\nlearner.fine_tune(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.083923\n0.330193\n0.081081\n00:37\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.478743\n0.215766\n0.081081\n00:42\n\n\n1\n0.323527\n0.146788\n0.054054\n00:42\n\n\n2\n0.256091\n0.159724\n0.054054\n00:41\n\n\n3\n0.196183\n0.199050\n0.081081\n00:48\n\n\n4\n0.156227\n0.217985\n0.081081\n00:44\n\n\n\n\n\nWith 5 epoc, our model has a 8% error. It is good to see where our model is making wrong preditions. A confusion matrix can tell us just that.\n\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\nOne Golden Retriever image and two Labrador images were predicted incorrectly. We can sort our images by loss and see why our model was making incorrect preditions on those images.\n\ninterp.plot_top_losses(3, nrows=3)\n\n\n\n\n\n\n\n\n\n\n\nThe first picture looks like a Golden Retriever and the secomd picture looks like a Labrador to me. It could be our labels are wrong. The last image is a puppy which is very hard to tell whether it is a Golden Retriever or a Labrador. By looking at these pictures, our model’s performance is acutally pretty good.\nNow we have a model. It is time to use it for prediction."
  },
  {
    "objectID": "how_to_create_a_binary_class_classifier.html#deploy-our-model",
    "href": "how_to_create_a_binary_class_classifier.html#deploy-our-model",
    "title": "How to Create an Image Classifer",
    "section": "Deploy Our Model",
    "text": "Deploy Our Model\nWe can’t expect our users to use Jupyter Notebook, so we need to export our model and host it on the web.\n\nlearner.export('model.pkl')\n\nWe have exported our model into a model.pkl file. Next we need to host our model on a web service, so others can use it. In this tutorial, we will use Hugging Face to host our model.\nFirst, we need to create a new space on Hugging Face. \nNext we need to clone this space to our local computer.\ngit clone https://huggingface.co/spaces/Jerryweijin/Labrador_and_Golden_Retriever_Classifier\nOnce the space is cloned, copy the model.pkl file to the Hugging Face diretory. We will need to upload our model along with other files to Hugging Face.\nNext, we need to create an app.py file in the Hugging Face directory. This is the file that runs our model, presents an UI to our users, and display the predition result. If you are a web developer, you can use HTML and Javascript to build whatever UI you want, but since I am not, I will use Gradio to build the UI.\n# app.py\nfrom fastai.vision.all import *\nimport gradio as gr\n\n# Load our model\nlearn = load_learner('model.pkl')\n\n# A function to call when an image is submited\ncategories=('Golden Retriever', 'Labrador')\ndef predict(img):\n    pred,idx,prob = learn.predict(img)\n    return dict(zip(categories, map(float, prob)))\n\n# Define a gradio interface\nimage = gr.inputs.Image((224,224)) # Input to our model is an image\nlabel = gr.outputs.Label() # Output of our model is a label\n\n# Gradio will build the UI for users to upload an image and display the prediction result \ninterface = gr.Interface(fn=predict, inputs=image, outputs=label)\ninterface.launch()\nLastly, we need a requirement.txt file. Hugging Face will look at this file and install all the required packages for us. For this project, we only need to install fastai, so our requirement.txt file only has one item.\n# requirements.txt\nfastai\nYou Hugging Face directory now should have 3 files: model.pkl, app.py, and requirement.txt. Commit the changes and push the files to Hugging Face space.\ngit commit -m \"Initial Commit\"\ngit push\nNow go to the Hugging Face space and you should see the app is being built. Once building is completed, you will see your app running on Hugging Face. Try to upload a new image and see your model work in a live environment.\n\n\n\nHugging_Face_App.png\n\n\nIn this tutorial, I have shown how to build a simple binary image classifier using fastai and host the model on Hungging Face. In the next tutorial, we will expand on this model to do multi-class classification. We will classify pet breeds."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Journal",
    "section": "",
    "text": "If you write codes using Jupyter Notebook, you probably want to check out nbdev. nbdev is a tool developed by fastai to streamline your development in notebooks. You do all your developmemnt, documentation, and unit testing in notebooks. nbdev help you convert your notebooks into python scripts, build them into a package, and upload the package to PyPI or Anaconda. nbdev can also turn your notebooks into a documentation website and host them on Github Pages. When using Git, Git may damage your notebooks and make them unopenable. nbdev solves these problems for you, so that you can freely use Git and Github.\nHow to use nbdev\nHello"
  },
  {
    "objectID": "index.html#how-to-use-nbdev",
    "href": "index.html#how-to-use-nbdev",
    "title": "Learning Journal",
    "section": "",
    "text": "If you write codes using Jupyter Notebook, you probably want to check out nbdev. nbdev is a tool developed by fastai to streamline your development in notebooks. You do all your developmemnt, documentation, and unit testing in notebooks. nbdev help you convert your notebooks into python scripts, build them into a package, and upload the package to PyPI or Anaconda. nbdev can also turn your notebooks into a documentation website and host them on Github Pages. When using Git, Git may damage your notebooks and make them unopenable. nbdev solves these problems for you, so that you can freely use Git and Github.\nHow to use nbdev\nHello"
  },
  {
    "objectID": "image_regression.html",
    "href": "image_regression.html",
    "title": "Image Regression",
    "section": "",
    "text": "Image Regression means the input of our model is an image and the output is a number. One application is to input an image and predict the key points of a human’s head. Applications like Snapchat use this technology to overlay a mast on a human’s face. In this tutorial we will train a simplifed model that only predict the center of a human’s head.\nFirst, we need to import the dataset we need.\n\nfrom fastai.vision.all import *\n\ndata_path = untar_data(URLs.BIWI_HEAD_POSE)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  Expected in: /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  warn(f\"Failed to load image Python extension: {e}\")\n\n\nLet’s look at how this dataset is structured\n\n\n\nBIWI_HEAD_POSE_Structure.png\n\n\nEach person’s files are stored in a separate folder. The first persion’s files are stored in the 01 folder , the second persion’s files are stored in the 02 folder, and so on. We don’t need the .obj files for our model, so we will ignore these files for now. Inside each person’s folder, we have some jpeg files, some pose files, and a .cal file. The jpeg files are the images we feed into our model. The post files are used to label our image. Let’s look at an image from this dataset.\n\nimage_paths = get_image_files(data_path)\nimg = PILImage.create(image_paths[0])\nimg.to_thumb(200)\n\n\n\n\nWe can start building our dataset step by step\n\ndata = DataBlock(get_items=get_image_files).datasets(data_path)\nx, y = data.train[0]\nx, y\n\n(Path('/Users/jerryyu/.fastai/data/biwi_head_pose/08/frame_00672_rgb.jpg'),\n Path('/Users/jerryyu/.fastai/data/biwi_head_pose/08/frame_00672_rgb.jpg'))\n\n\nWe got a path for x which is good. Later on when we pass ImageBlock to blocks, fastai will help us convert it to a PIL image. We need to define some functions to get the correct values for y. First, we define a function to get the pose file path based on the jpeg file path.\n\ndef img2pose(image_path):\n    return Path(str(image_path)[:-7]+'pose.txt')\n\n\nimg2pose(x)\n\nPath('/Users/jerryyu/.fastai/data/biwi_head_pose/08/frame_00672_pose.txt')\n\n\nNow we know where to find the pose file, we can define a function to get the center of the head in an image.\n\ncal = np.genfromtxt(data_path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\nget_ctr(x)\n\ntensor([350.7517, 275.2747])\n\n\nWe can then pass get_ctr to get_y in our DataBlock call.\n\ndata = DataBlock(\n    get_items=get_image_files,\n    get_y = get_ctr\n).datasets(data_path)\nx, y = data.train[0]\nx, y\n\n(Path('/Users/jerryyu/.fastai/data/biwi_head_pose/15/frame_00465_rgb.jpg'),\n tensor([348.8435, 303.9288]))\n\n\nNow we can specify our input and output blocks so that fastai can do the proper conversion for us.\n\ndata = DataBlock(\n    blocks = (ImageBlock, PointBlock),\n    get_items = get_image_files,\n    get_y = get_ctr\n).datasets(data_path)\nx, y = data.train[0]\nx, y\n\n(PILImage mode=RGB size=640x480, TensorPoint([[357.7530, 306.4343]]))\n\n\nWe specify the output is a PointBlock so that fastai knows that the labels represent coordinates and when we do image augmentation, fastai will do the same augmentation to the labels. We are now ready to create our dataloaders.\nOne thing we need to pay attention to is we can’t randomly split the data to create the training and validation set because the same person will appear in both datasets. We want the validation set to contain people our model has never seen during training. We need to specify a splitter in our DataBlock. We will also add augmentation to help train our model and resize our images to make training faster.\n\ndls = DataBlock(\n    blocks = (ImageBlock, PointBlock),\n    get_items = get_image_files,\n    get_y = get_ctr,\n    splitter = FuncSplitter(lambda image_path: image_path.parent.name == '13'),\n    batch_tfms = aug_transforms(size=(240,320))\n).dataloaders(data_path)\ndls.show_batch(max_n=3)\n\n\n\n\nWe are now ready to create our learner\n\nlearner = vision_learner(dls, resnet18, y_range=(-1,1))\nlearner.fine_tune(2)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.129507\n0.008684\n34:23\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.007625\n0.002542\n44:54\n\n\n1\n0.003237\n0.000435\n45:08\n\n\n\n\n\nWe specify y_range to let fastai know that the label is between -1 and 1 as coordinate in fastai is between -1 and 1. Let’s look at some of the preditions on our validaiton set.\n\nlearner.show_results(max_n=2)\n\n\n\n\n\n\n\n\n\n\n\nThat looks pretty accurate. Let’s see how well the model can generalize on images downloaded from the Internet. test_person.jpg and test_persion_2.jpg are images I downloaded from pexels\n\nprediction = learner.predict('../images/test_person.jpg')\nprediction\n\n\n\n\n\n\n\n\n(TensorPoint([[179.6764, 114.6520]]),\n TensorBase([ 0.1230, -0.0446]),\n TensorBase([ 0.1230, -0.0446]))\n\n\nThere are two ways to show the output on the test image. One way is to use matplotlib directly. The prediction will output the coordinate of the key point and the coordinate is based on the resized image, so before plotting the prediction on the test image, we need to resize the image.\n\nimport matplotlib.pyplot as plt\n\nrsz = Resize((240, 320))\nimage = PILImage.create('../images/test_person.jpg')\nrsz_image = rsz(image)\nx = prediction[0][0][0]\ny = prediction[0][0][1]\nplt.imshow(rsz_image)\nplt.plot(x, y, 'og', markersize=5)\n\n\n\n\nThe second way is to use the show_image method from fastai.\n\nprediction = learner.predict('../images/test_person_2.jpg')\nprediction\n\n\n\n\n\n\n\n\n(TensorPoint([[183.4863, 113.3780]]),\n TensorBase([ 0.1468, -0.0552]),\n TensorBase([ 0.1468, -0.0552]))\n\n\n\nrsz = Resize((240, 320))\nimage = PILImage.create('../images/test_person_2.jpg')\nax = plt.subplot()\nshow_image(rsz(image), ax=ax)\nx = prediction[0][0][0]\ny = prediction[0][0][1]\nplt.plot(x, y, 'og', markersize=5)\n\n\n\n\nOur model did very well on the validation set but not well on test images downloaded from the web. This is a problem of overfitting. Looking at our training data, the head position of all the images are relatively center. Also all the images were taken from the same camera in the same room. To improve our model, we can add more variation to our training data like changing the head location in an image or use images taken from different cameras and different backgrounds.\n\nimport matplotlib.pyplot as plt\nimport numpy\n\nx = np.arange(10)\ny = x * 2\nplt.plot(x, y, color=(0.2, 0.9, 0.6))\n\n\n\n\n\nlearner = load_learner('model.pkl')\n\ndef predict(image):\n    # Run the model to get the prediciton\n    prediction = learner.predict(image)\n    \n    # Create a figure to show the prediction\n    rsz = Resize((240, 320))\n    image = PILImage.create(image)\n    rsz_image = rsz(image)\n    x = prediction[0][0][0]\n    y = prediction[0][0][1]\n    fig, ax = plt.subplots()\n    plt.imshow(rsz_image)\n    plt.plot(x, y, 'og', markersize=5)\n    \n    # Save the figure to a np array\n    data = fig2data(fig)\n    \n    return data\n\ninterface = gr.Interface(fn=predict, inputs='image', outputs='image')\ninterface.launch()\n\n\nThanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\nRunning on local URL:  http://127.0.0.1:7874\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearner.export('model.pkl')"
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html",
    "href": "build_a_image_classifier_from_scratch.html",
    "title": "Build a Image Classifier From Scratch",
    "section": "",
    "text": "First, we need some data to work with. For this tutorial, we will use the Imagenette dataset from fastai. This is a subset of the Imangenet dataset. It selects 10 very different classess from the full Imagenet dataset.\nfrom fastai.vision.all import *\ndata_path = untar_data(URLs.IMAGENETTE)\n\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n  Referenced from: &lt;9E417059-FF86-3C93-B8D5-8A071C12B008&gt; /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torchvision/image.so\n  Expected in:     &lt;3BC89DD9-AAC2-3081-A655-ECB993924628&gt; /Users/jerryyu/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n  warn(f\"Failed to load image Python extension: {e}\")\nLet’s take a look at the structure of this dataset so that we can build our datablock\nTraining and validation data is separated in its own folder. Inside train and val, there are 10 classes each has its own folder. The folder name is the class label. Images of each class are stored in its coressponding class folder.\ndblocks = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items = get_image_files, \n    get_y = parent_label, \n    item_tfms = Resize(460), \n    batch_tfms = aug_transforms(size=224, min_scale=0.75))\n\ndls = dblocks.dataloaders(data_path, bs=64)\ndls.show_batch()\nThe labels are not very readable. Let’s change them to something easier to understand. Let’s create a function to map the original label to a human readable label and use this function in our DataBlock. We also want to specify a splitter to use images in the train folder for traning data and images in the val folder for validation data.\nlabel_lookup = dict(\n    n01440764='tench',\n    n02102040='English springer',\n    n02979186='cassette player',\n    n03000684='chain saw',\n    n03028079='church',\n    n03394916='French horn',\n    n03417042='garbage truck',\n    n03425413='gas pump',\n    n03445777='golf ball',\n    n03888257='parachute'\n)\n\ndef get_label(image_path):\n    label = parent_label(image_path)\n    return label_lookup[label]\n    \ndblocks = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items = get_image_files, \n    get_y = get_label, \n    splitter = GrandparentSplitter(train_name='train', valid_name='val'),\n    item_tfms = Resize(460), \n    batch_tfms = aug_transforms(size=224, min_scale=0.75))\n\ndls = dblocks.dataloaders(data_path, bs=64)\ndls.show_batch()\nNow our data is ready. Let’s create a model from scratch and get a baseline of our model performance\nmodel = xresnet50(pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.fit_one_cycle(5, lr_max=0.003)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.737712\n2.343394\n0.381911\n1:12:19\n\n\n1\n1.290873\n1.469322\n0.569427\n1:17:06\n\n\n2\n1.033922\n1.083741\n0.661656\n1:12:29\n\n\n3\n0.779879\n0.699522\n0.780127\n1:06:40\n\n\n4\n0.634020\n0.603678\n0.814268\n1:08:30\nThis is a good baseline for our model performance. Let’s find ways to improve our model. First we will look at normalization."
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#normailizaiton",
    "href": "build_a_image_classifier_from_scratch.html#normailizaiton",
    "title": "Build a Image Classifier From Scratch",
    "section": "Normailizaiton",
    "text": "Normailizaiton\nNormalization means bringing values to a similar scale. Normalization improve the performance and training stability of the model. We will normalize our data to the same mean and standard deviation of the Imagenet dataset since our data is a subsut of the Imagenet dataset.\n\ndblocks = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items = get_image_files, \n    get_y = get_label, \n    splitter = GrandparentSplitter(train_name='train', valid_name='val'),\n    item_tfms = Resize(460), \n    batch_tfms = [*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n\ndls = dblocks.dataloaders(data_path, bs=64)\nx,y = dls.one_batch()\nx.mean(dim=[0,2,3]), x.std(dim=[0,2,3])\n\n(TensorImage([-0.0023,  0.0931,  0.2008]),\n TensorImage([1.2750, 1.2831, 1.3656]))\n\n\n\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.fit_one_cycle(5, lr_max=0.003)"
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#progressive-resizing",
    "href": "build_a_image_classifier_from_scratch.html#progressive-resizing",
    "title": "Build a Image Classifier From Scratch",
    "section": "Progressive Resizing",
    "text": "Progressive Resizing\nTo get better result, we can also use pregressive resizing. What that mean is we start training with smaller images so that we can train faster. We gradually increase the size of the images so that we can include more detail in our training data.\n\ndef get_dls(size, bs):\n    dblocks = DataBlock(\n        blocks=(ImageBlock, CategoryBlock), \n        get_items = get_image_files, \n        get_y = get_label, \n        splitter = GrandparentSplitter(train_name='train', valid_name='val'),\n        item_tfms = Resize(460), \n        batch_tfms = [*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)])\n    return dblocks.dataloaders(data_path, bs=bs)\n\n\ndls = get_dls(128, 64)\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.fit_one_cycle(4, lr_max=0.003)\n\n\n\n\nNext we increase the size of the images and use fine_tune to make our model adopt the new images. We use fine_tune because this is like transfer learning where we take a pretrained model and fine tune it for new data.\n\ndls = get_dls(224, 64)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.fine_tune(5, max_lr=0.001)"
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#test-time-augmentation",
    "href": "build_a_image_classifier_from_scratch.html#test-time-augmentation",
    "title": "Build a Image Classifier From Scratch",
    "section": "Test Time Augmentation",
    "text": "Test Time Augmentation\nSo far we have only applied augmentation to our training dataset. We can do the some thing to our validation set as well. By doing that we will get a prediction for each augmented imange and we can average them to get our final prediction.\n\npreds, targets = learner.tta()\naccuracy(preds, targets)"
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#mixup",
    "href": "build_a_image_classifier_from_scratch.html#mixup",
    "title": "Build a Image Classifier From Scratch",
    "section": "Mixup",
    "text": "Mixup\nMixup is an augmentation technique where we create a new input by mixing two images by a random weight. In pseudocode, it works like this:\nimage1,target1 = dataset[randint(0,len(dataset)]  image2,target2 = dataset[randint(0,len(dataset)] lam = random_float(0.5, 1) //This is the ramdom weight  new_input = lam * image1 + (1-lam) * image2 new_target = lam * target1 + (1-lam) * target2\nNote from the pseudocode above, targets are one-hot encoded. The new target is a weighted sum of the targets of the two images. The loss function will then change to\nnew_loss = loss_func(preds, new_target)\nIn practice, tartgets are integers and not one-hot encoded. If targets are not one-hot encoded, the above loss funcction can be rewritten as:\nnew_loss = lam * loss_func(preds, y1) + (1-lam) * loss_func(preds, y2).\nwhere y1 and y2 are the integer labels of image1 and image2 respetively.\nIn fastai, we don’t need to worry about all these implementation detail. All we need to do is to pass mixup to the cbs parameter when creating the learner.\n\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=Mixup())\nlearner.fit_one_cycle(5, lr_max=0.003)\n\nIt requires more more epocs to see the improvements from mixup."
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#label-smoothing",
    "href": "build_a_image_classifier_from_scratch.html#label-smoothing",
    "title": "Build a Image Classifier From Scratch",
    "section": "Label Smoothing",
    "text": "Label Smoothing\nOur model has the tendency to go extreme. Even an outpout of 0.999 is not good enough. There is still a gradient and our model will still try to get even closer to 1. However, 1 is a value we can never get in a Sigmoid or Softmax function. Therefor the more epocs we train, the more extreme our model becomes. To solve that, we can use label smoothing.\nIn theoretical expression, labels of classification models are one-hot encoded. In practice, labels are not one-hot encoded to save memory but the loss is the same loss. In the settings of one-hot encoded labels, label smoothing changes 1 to a value slightly than 1 and changes 0 to a value slightly greater than 0.\n\nChange all 0 to 0 + epsilon / N where N is the number of classes and epsilon is a parameter we can set (usually 0.1 which means we are 10% unsure of our labels)\nChange 1 to 1 - epsilon * (N-1) / N\n\nBy doing this all the labels still add up to 1. To use label smoothing in Fastai, we only need to change the loss function.\n\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)\nlearner.fit_one_cycle(5, lr_max=0.003)\n\nLabel smoothing does not work well with mixup because label smoothing has modified the labels by some factor."
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#finding-the-best-learning-rate",
    "href": "build_a_image_classifier_from_scratch.html#finding-the-best-learning-rate",
    "title": "Build a Image Classifier From Scratch",
    "section": "Finding the Best Learning Rate",
    "text": "Finding the Best Learning Rate\nHaving a good learning rate is important for our training. If the learning rate is too low, it will take a long time to train our model and we may overfit our training data. If the learning rate is too high, our loss may get bigger and bigger instead of descresing. To find the best learning rate, we can use the lr_find method.\n\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.lr_find()\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n    \n      \n      4.08% [6/147 02:55&lt;1:08:40 2.8455]"
  },
  {
    "objectID": "build_a_image_classifier_from_scratch.html#weight-decay",
    "href": "build_a_image_classifier_from_scratch.html#weight-decay",
    "title": "Build a Image Classifier From Scratch",
    "section": "Weight Decay",
    "text": "Weight Decay\nAnother name for weight decay is regularization. It means adding sum of square of all the parameters to the loss function.\nloss_with_wd = loss + wd * (parameters ** 2).sum()\nWhere wd is a parameter to control the amount of regularation applied to our model. The effect of adding the regularation term is to make the value of the parameters smaller hence the name weight decay. Smaller weights will lead to a smoother hypothesis function and less prone to overfit.\nTo apply weight decay in fastai, simply pass a value to wd in the fit_one_cycle call.\n\nmodel = xresnet50(n_out=dls.c, pretrained=False)\nlearner = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearner.fit_one_cycle(5, lr_max=0.003, wd=0.1)"
  },
  {
    "objectID": "how_to_use_nbdev.html",
    "href": "how_to_use_nbdev.html",
    "title": "How to Use nbdev",
    "section": "",
    "text": "nbdev is a python tool developed by fastai to streamline your developemnt with Jupyter Notebook. Its main features are:\n\nYou write your codes, documentation, and test cases all in notebooks.\nnbdev exports the code parts of notebooks into python scripts, build them into a package, and upload the package to PyPI or Anaconda.\nnbdev converts the documentation parts of notebooks into a website and host the website on Github Pages.\nnbdev does unit testing by running your test cases\nnbdev makes Jupyter Notebook Git friendly. It solves all the problems when using Jupyter Notebook with Git.\nnbdev allows you to set up Actions on Github to support continuous integration\n\nNow you know what nbdev is and the benefits of using nbdev, let’s see how to use it."
  },
  {
    "objectID": "how_to_use_nbdev.html#what-is-nbdev",
    "href": "how_to_use_nbdev.html#what-is-nbdev",
    "title": "How to Use nbdev",
    "section": "",
    "text": "nbdev is a python tool developed by fastai to streamline your developemnt with Jupyter Notebook. Its main features are:\n\nYou write your codes, documentation, and test cases all in notebooks.\nnbdev exports the code parts of notebooks into python scripts, build them into a package, and upload the package to PyPI or Anaconda.\nnbdev converts the documentation parts of notebooks into a website and host the website on Github Pages.\nnbdev does unit testing by running your test cases\nnbdev makes Jupyter Notebook Git friendly. It solves all the problems when using Jupyter Notebook with Git.\nnbdev allows you to set up Actions on Github to support continuous integration\n\nNow you know what nbdev is and the benefits of using nbdev, let’s see how to use it."
  },
  {
    "objectID": "how_to_use_nbdev.html#install-nbdev",
    "href": "how_to_use_nbdev.html#install-nbdev",
    "title": "How to Use nbdev",
    "section": "Install nbdev",
    "text": "Install nbdev\nBefore you can use nbdev, you need to install it. If you use conda, type\nconda install -c fastai nbdev\nIf you use pip, type\npip install nbdev"
  },
  {
    "objectID": "how_to_use_nbdev.html#create-a-github-repo",
    "href": "how_to_use_nbdev.html#create-a-github-repo",
    "title": "How to Use nbdev",
    "section": "Create a Github Repo",
    "text": "Create a Github Repo\n\n\n\n\n\n\nNote\n\n\n\nUsing Github is not required to use nbdev. If you don’t use Github, you can skip this step.\n\n\nGo to your Github account and create a new repo. I created a repo called nbdev_demo for this tutorial. You don’t need to check the Add a README file checkbox as nbdev will create the README file for you.\n\n\n\nCreate_Github_Repo\n\n\nYour repo is now empty\n\n\n\nEmpty_Repo"
  },
  {
    "objectID": "how_to_use_nbdev.html#clone-your-repo",
    "href": "how_to_use_nbdev.html#clone-your-repo",
    "title": "How to Use nbdev",
    "section": "Clone Your Repo",
    "text": "Clone Your Repo\nNext you clone the repo to your computer. I used Github Desktop as I prefer a GUI interface. You can also use the command line to clone the repo.\n\n\n\nClone Repo"
  },
  {
    "objectID": "how_to_use_nbdev.html#initialize-the-project-with-nbdev",
    "href": "how_to_use_nbdev.html#initialize-the-project-with-nbdev",
    "title": "How to Use nbdev",
    "section": "Initialize the Project with nbdev",
    "text": "Initialize the Project with nbdev\nAftering cloning the repo, the next thing to do is to initialize the project with nbdev. Go to the repo\ncd nbdev_demo\nand run\nnbdev_new\nnbdev will add some files to your repo.\n\n\n\nFiles\n\n\nThe nbs folder stores all your notebooks. nbdev creates a folder with the same name as your repo. In my case, the folder is called nbdev_demo. This folder stores all the python scripts that are exported from the notebooks. The _proc folder stores the documentation files for your site (e.g. html files) that are generated from your notebooks using Quarto. settings.ini and setup.py are used to generate your python package. You can also see that nvdev adds a README file and a LICENSE file for you.\nAfter running nbdev_new, I usually run nbdev_install_hooks right after because I know I will be using Git.\nnbdev_install_hooks\nYou can think of hooks as an extension to nbdev. Jupyter notebooks are not Git friendly. Git is designed to use with plain text files. Jupyter Notebook uses JSON as its format. When using Jupyter Notebook with Git, there will be some problems. Citing from the nbdev official website, here is what hooks do for you:\n\n\nFix broken notebooks due to git merge conflicts so that they can be opened and resolved directly in Jupyter.\nEach time you save a Jupyter notebook, automatically clean unwanted metadata to remove unnecessary changes in pull requests and reduce the chance of git merge conflicts.\nAutomatically trust notebooks in the repo so that you can view widgets from collaborators’ commits. For this reason, you should not install hooks into a repo you don’t trust."
  },
  {
    "objectID": "how_to_use_nbdev.html#make-your-first-commit",
    "href": "how_to_use_nbdev.html#make-your-first-commit",
    "title": "How to Use nbdev",
    "section": "Make Your First Commit",
    "text": "Make Your First Commit\nNow your repo is set up. We can make the first commit and push to Github.\n\n\n\nFirst_Commit\n\n\nNavigate to the Action tab and you should see two workflows have been executed. These workflows are added by nbdev to help continous integration.\n\n\n\nWorkflow_Run\n\n\nnbdev hosts your documentation site on Github Pages. To enable Pages for your project, go to Settings and click Pages from the left menu. Select the branch gh-pages and save the change.\n\n\n\nEnable_Github_Page\n\n\nYou should see another workflow has been executed.\n\n\n\nPage_Build_Workflow\n\n\nYou can check your documentation site on [your_username].github.io/[your_repo_name]\n\n\n\nDoc_Site"
  },
  {
    "objectID": "how_to_use_nbdev.html#install-your-package",
    "href": "how_to_use_nbdev.html#install-your-package",
    "title": "How to Use nbdev",
    "section": "Install Your Package",
    "text": "Install Your Package\nIf your project consists of multiple python scripts, you probably want to import them into your notebooks. To do that install your package using the editable mode.\npip install -e '.[dev]'\nThat way when you change your source files, you don’t need to reinstall the package. Simply restart the notebook kernal to see the changes take effect."
  },
  {
    "objectID": "how_to_use_nbdev.html#make-your-first-edit",
    "href": "how_to_use_nbdev.html#make-your-first-edit",
    "title": "How to Use nbdev",
    "section": "Make Your First Edit",
    "text": "Make Your First Edit\nNow it is time to do development on the notebooks. To demonstrate, I have updated the foo function in 00_core.ipynb to print out Hello World and called this function in index.ipynb.\n\n\n\nFirst_edit\n\n\nThen commited and pushed to Github. You can see the site was updated.\n\n\n\nDoc_Site_Updated\n\n\nChecking on the Action tab, there was a worflow failed to execute. What was wrong?\n\n\n\nSecond_Commit"
  },
  {
    "objectID": "how_to_use_nbdev.html#run-nbdev_prepare-before-every-commit",
    "href": "how_to_use_nbdev.html#run-nbdev_prepare-before-every-commit",
    "title": "How to Use nbdev",
    "section": "Run nbdev_prepare Before Every Commit",
    "text": "Run nbdev_prepare Before Every Commit\nTo avoid the error, remember to run nbdev_prepare before every commit. nbdev_prepare bundles 4 things for you. The error was caused by not clearning the notebooks.\n\nnbdev_export: Builds the .py modules from Jupyter notebooks\nnbdev_test: Tests your notebooks\nnbdev_clean: Cleans your notebooks to get rid of extreanous output for Github\nnbdev_readme: Updates README.md from your index notebook.\n\nAfter running nbdev_prepare, all workflows were successfully executed.\n\n\n\nThird_Commit"
  },
  {
    "objectID": "how_to_use_nbdev.html#learn-more",
    "href": "how_to_use_nbdev.html#learn-more",
    "title": "How to Use nbdev",
    "section": "Learn More",
    "text": "Learn More\nnvdev provides a lot more features than what I can cover in this tutorial. For more info, check out nbdev’s documentation website."
  }
]